{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/filchyboy/DS-Unit-4-Sprint-1-NLP/blob/master/Copy_of_Unit_4_Sprint_1_Study_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nd2OOOVXxXS1"
   },
   "source": [
    "This study guide should reinforce and provide practice for all of the concepts you have seen in Unit 4 Sprint 1. There are a mix of written questions and coding exercises, both are equally important to prepare you for the sprint challenge as well as to be able to speak on these topics comfortably in interviews and on the job.\n",
    "\n",
    "If you get stuck or are unsure of something remember the 20 minute rule. If that doesn't help, then research a solution with google and stackoverflow. Only once you have exausted these methods should you turn to your Team Lead - they won't be there on your SC or during an interview. That being said, don't hesitate to ask for help if you truly are stuck.\n",
    "\n",
    "Have fun studying!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpvInKdXekFi"
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6bS8AhBZ86H"
   },
   "source": [
    "When completing this section, try to limit your answers to 2-3 sentences max and use plain english as much as possible. It's very easy to hide incomplete knowledge and undertanding behind fancy or technical words, so imagine you are explaining these things to a non-technical interviewer.\n",
    "\n",
    "1. What does it mean to Tokenize?\n",
    "\n",
    "```\n",
    "In a lexical analysis tokenization refers to breaking up a corpus into individual words, or in some case other units such as sentences, letters, etc.\n",
    "\n",
    "```\n",
    "\n",
    "2. How would you go about tokenizing the data?\n",
    "\n",
    "```\n",
    "Manually or by the use of functional libraries one would split the words out of a corpus by the use of delimiters such as the spaces between words in a sentence.\n",
    "\n",
    "```\n",
    "\n",
    "3. How would you create a Vector of those tokens?\n",
    "\n",
    "```\n",
    "I would use a token processing pipeline such as that fforded by Spacy to break a corpus down into its constituent tokens.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sj5-1Nnkpej6"
   },
   "source": [
    "### What to study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vz6CusBmpDgz"
   },
   "source": [
    "tokenize\n",
    "```\n",
    "**token**: an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing\n",
    "\n",
    "### The attributes of good tokens\n",
    "\n",
    "* Should be stored in an iterable data structure\n",
    "  - Allows analysis of the \"semantic unit\"\n",
    "* Should be all the same case\n",
    "  - Reduces the complexity of our data\n",
    "* Should be free of non-alphanumeric characters (ie punctuation, whitespace)\n",
    "  - Removes information that is probably not relevant to the analysis\n",
    "\n",
    "```\n",
    "vector representation\n",
    "```\n",
    "\n",
    "Processing text data to prepare it for machine learning models often means translating the information from documents into a numerical format. This is \"vectorizing\" tokenized documents. It essentially translates lexical units into numbers by generating dataframes of number showing counts of token, etc.\n",
    "\n",
    "```\n",
    "classification model\n",
    "```\n",
    "\n",
    "By the use of training input data models are developed with the intent to predict class labels and categories with new data.\n",
    "\n",
    "```\n",
    "topic model\n",
    "```\n",
    "\n",
    "A type of statistical model which attempts to find hidden semantic structures in a corpus by finding clusters of lexical information.\n",
    "\n",
    "```\n",
    "piepline object\n",
    "```\n",
    "\n",
    "A pipeline object allows a series of modeling types of operations to be applied seqentially to a set of data.\n",
    "\n",
    "```\n",
    "named entity recognition\n",
    "```\n",
    "\n",
    "Within an NLP classification model such as Spacy named entity recognition attempts to establish which tokens belong to lexical classes such as names, money denominations, acronyms, business titles, and such.\n",
    "\n",
    "```\n",
    "vectorization methods\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "generator object\n",
    "```\n",
    "A generator object is a simple way of creating interators. Generators can abstract sets of data and iterate over it in a similar way that one would iterate over single bits of data.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUQaIwbceohq"
   },
   "source": [
    "## Practice Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WAeFeS_xIkdk"
   },
   "source": [
    "Use your own Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0myqx5w_InDM"
   },
   "source": [
    "Analyze the corpus of text using text visualizations of token frequency. Try cleaning the data as much as possible. Try the following techniques:\n",
    "\n",
    "Lemmatization\n",
    "Custom stopword removal\n",
    "Keep in mind the attributes of good tokens. Once you have a solid baseline, layer in the star rating in your visualization(s). Key part of this assignment - produce a write-up of the attributes of the best and worst coffee shops. Based on your analysis, what makes the best the best and the worst the worst. Use graphs and numbesr from your analysis to support your conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4jnYgnFjP6eE"
   },
   "source": [
    "Overall Word / Token Count\n",
    "View Counts by Rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9hFYrmqQlLA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCbET5ioQlmQ"
   },
   "source": [
    "Can visualize the words with the greatest difference in counts between 'good' & 'bad'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lwNwPn5nQowi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2hq-PhcTQph7"
   },
   "source": [
    "Use Spacy to tokenize the listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bD4BtmWNI6lr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNOzs60uI70-"
   },
   "source": [
    "Use Scikit-Learn's CountVectorizer to get word counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsWsuYwXRRP3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_teLVI02JCzH"
   },
   "source": [
    "Visualize the most common word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tWMUVjLtJATI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZ3lrC03JEFg"
   },
   "source": [
    "Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbSDe4RyJJZS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NSYkLcnJG4R"
   },
   "source": [
    "Create a NearestNeighbor Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JkRZOpnHJOGq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wc-bZUQQJO3O"
   },
   "source": [
    "Try different visualizations for words and frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6NZfHLdJRPU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pe2RMxvEJRW6"
   },
   "source": [
    "Fit a Gensim LDA topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yC4s0KtGJc1x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of Unit 4 Sprint 1 - Study Guide.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
